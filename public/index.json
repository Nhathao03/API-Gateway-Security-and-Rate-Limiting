[
{
	"uri": "//localhost:1313/",
	"title": "API Gateway Security and Rate Limiting",
	"tags": [],
	"description": "",
	"content": "Work with Amazon API Gateway Security and Rate Limiting - Session Manager Overall In this lab, you\u0026rsquo;ll learn the basics and practice of Amazon API Gateway Security and Rate Limiting - Session Manager . Perform creating public and private instance connections.\n"
},
{
	"uri": "//localhost:1313/3-configcognito/3.3-authenticationandstorage/3.3.1-authenticationwithamplify/",
	"title": "Authentication with Anplify",
	"tags": [],
	"description": "",
	"content": " Run the following command at the root of the application you cloned to add authentication to the application: amplify add auth Select follow the informations below:\nDo you want to use the default authentication and security configuration? Default configuration Warning: you will not be able to edit these selections. How do you want users to be able to sign in? Username Do you want to configure advanced settings? No, I am done. Run the following command to update cloud resources: amplify push Navigate to the CloudFormation console to check if the stack has been created. Click id of UserPool to open dashboard of Cognito User Pool Run the following command to start the application: npm start Click Sign up to register a new account. Enter user information: Username, for example: admin Enter your email. Password, for example: Admin123 Click **Sign up Navigate to Cognito User Pool console, select User tab you will see a registered but unconfirmed user Open your email to get the verification code that was sent automatically. Enter the verification code into the app and click Submit Navigate to Cognito User Pool console, click the Refresh icon and you will see that the user is confirmed Log in to the app with the account you just registered with. Successful login. "
},
{
	"uri": "//localhost:1313/2-deloydatabase/2.1-createdynamodbtable/",
	"title": "Create DynamoDB table",
	"tags": [],
	"description": "",
	"content": "CREATE DYNAMODB TABLE Open DynamoDB console Click Create table Enter table name: Documents Enter Parition key is user_id Enter Sort key is file In Table setting section, select Customsize setting Keep DynamoDB Standard for Table class Select On-demand for Capacity mode Scroll to the bottom of the page, click Create table "
},
{
	"uri": "//localhost:1313/2-deloydatabase/2.2-createlambdafunctions/2.2.1-createlistingfunction/",
	"title": "Create listing function",
	"tags": [],
	"description": "",
	"content": "In this section we will create a function to list the documents stored in the DynamoDB table by the user‚Äôs id.\nOpen AWS Lambda console Click Create function Enter function name: list_documents Select Python 3.9 for Runtime Click Create function Enter the following code for the lambda_function.py file: import json import boto3 import os from decimal import * from boto3.dynamodb.types import TypeDeserializer dynamodb = boto3.client(\u0026#39;dynamodb\u0026#39;) serializer = TypeDeserializer() class DecimalEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, Decimal): return str(obj) return json.JSONEncoder.default(self, obj) def deserialize(data): if isinstance(data, list): return [deserialize(v) for v in data] if isinstance(data, dict): try: return serializer.deserialize(data) except TypeError: return {k: deserialize(v) for k, v in data.items()} else: return data def lambda_handler(event, context): table_name = os.environ[\u0026#39;TABLE_NAME\u0026#39;] user_id = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] print(user_id) docs = dynamodb.query( TableName=table_name, KeyConditionExpression=\u0026#34;user_id = :id\u0026#34;, ExpressionAttributeValues={ \u0026#34;:id\u0026#34;: { \u0026#39;S\u0026#39;: user_id } } ) format_data_docs = deserialize(docs[\u0026#34;Items\u0026#34;]) # TODO implement return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;headers\u0026#34;: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;GET,PUT,POST,DELETE, OPTIONS\u0026#34;, \u0026#34;Access-Control-Allow-Headers\u0026#34;: \u0026#34;Access-Control-Allow-Headers, Origin,Accept, X-Requested-With, Content-Type, Access-Control-Request-Method,X-Access-Token,XKey,Authorization\u0026#34; }, \u0026#34;body\u0026#34;: json.dumps(format_data_docs, cls=DecimalEncoder) } Then click Deloy The above code executes to get the user‚Äôs TABLE_NAME and id environment variables from the event. Then query to the DynamoDB table provided that the value of Partition key is equal to the user‚Äôs id. Then reformat the data returned after the query.\nWe need to add an environment variable to the function. Click the Configuration tab, then select Environment variables in the left menu. Press Edit Click Add environment variable Enter TABLE_NAME as key Enter the DynamoDB table name that you just created Click Save Next, add permissions for function to access DynamoDB table Click Permission on the left menu Click on the execution role of the function Expand the AWSLambdaBasicExecutionRole‚Ä¶ policy, then click Edit Click JSON. Copy the JSON below into the editor ,\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;dynamodb:Query\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:REGION:ACCOUNT_ID:table/Documents\u0026#34;\r} Replace REGION and ACCOUNT_ID with the region you create the table and your account id.\nClick Review policy Click Save changes "
},
{
	"uri": "//localhost:1313/4-frontendintergrationwithfrontend/4.1-deloyfrontend/",
	"title": "Deloy Front-end",
	"tags": [],
	"description": "",
	"content": "In the first step in this workshop, we will host the web application with S3 Static website hosting\nOpen Amazon S3 Console\nClick Create Bucket Enter bucket name, such as fcjdmswebstore-5801 Uncheck block from allowing public access\nCheck to I acknowledge that the current settings might result in this bucket and the objects within becoming public Click Create bucket button Click on created bucket Click Properties tab Scroll down to the bottom, click edit in Static web hosting pattern Select Enable to enable host web static on S3\nSelect Host a static website for Hosting type Enter index.html for Index document pattern Click Save changes After successfully enabling, please write down the path of the web After successful enable, please take note of the path of the web: Select Permissions tab Click Edit of Bucket policy pattern Copy the below code block to Policy {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::BUCKET_NAME/*\u0026#34;\r}\r]\r} Replace BUCKET_NAME with the bucket name you created, then click Save changes Open the src/component/Home/Upload.js file in the application‚Äôs source code directory and uncomment the code that calls the API to write data to DynamoDB. Next run the following command at the root of the project you downloaded. (FCJ-Serverless-DMS)\nyarn build\raws s3 cp build s3://BUCKET_NAME --recursive Replace BUCKET_NAME with the bucket name you created Result after uploading: Paste the web link you take notes into your web browser You have finished hosting your website on S3. In the next section, we update the lambda functions\n"
},
{
	"uri": "//localhost:1313/3-configcognito/3.1-introduceamplify/",
	"title": "Introduce Amplify",
	"tags": [],
	"description": "",
	"content": "Overview AWS Amplify is a toolset that helps developers quickly build and deploy web and mobile applications integrated with AWS services such as API, Authentication, Storage, and Hosting. Amplify simplifies backend management, provides frontend libraries, and offers ready-to-use hosting services for production applications.\nAuthentication with Amplify Authentication with Amplify makes it easy to integrate user authentication features into applications using Amazon Cognito as the backend. It supports sign-up, sign-in, forgot password, multi-factor authentication (MFA), and social sign-in (Google, Facebook, Apple), allowing developers to quickly implement secure user management.\nStorage with Amplify Storage with Amplify provides a solution for storing files (images, videos, documents) via Amazon S3. Data can be managed with privacy controls (public, protected, private), integrated with user authentication to control access, and supports secure file upload, download, and sharing.\n"
},
{
	"uri": "//localhost:1313/1-introduce/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Gi·ªõi thi·ªáu ƒë·ªÅ t√†i Trong b·ªëi c·∫£nh c√°c h·ªá th·ªëng hi·ªán ƒë·∫°i ng√†y c√†ng ph·ª• thu·ªôc v√†o API ƒë·ªÉ giao ti·∫øp gi·ªØa c√°c d·ªãch v·ª•, vi·ªác ƒë·∫£m b·∫£o an to√†n cho c√°c API tr·ªü th√†nh m·ªôt nhi·ªám v·ª• thi·∫øt y·∫øu. M·ªôt API kh√¥ng ƒë∆∞·ª£c b·∫£o v·ªá ƒë√∫ng c√°ch c√≥ th·ªÉ tr·ªü th√†nh ƒëi·ªÉm y·∫øu khi·∫øn c·∫£ h·ªá th·ªëng b·ªã t·∫•n c√¥ng, g√¢y m·∫•t d·ªØ li·ªáu, gi√°n ƒëo·∫°n d·ªãch v·ª• ho·∫∑c vi ph·∫°m tu√¢n th·ªß b·∫£o m·∫≠t.\nƒê·ªÅ t√†i ‚ÄúAPI Security Gateway v·ªõi Advanced Protection‚Äù h∆∞·ªõng ƒë·∫øn vi·ªác tri·ªÉn khai m·ªôt ki·∫øn tr√∫c API Gateway b·∫£o m·∫≠t cao c·∫•p tr√™n n·ªÅn t·∫£ng AWS, t√≠ch h·ª£p ƒë·∫ßy ƒë·ªß c√°c l·ªõp b·∫£o v·ªá hi·ªán ƒë·∫°i v√† quy chu·∫©n an ninh, bao g·ªìm:\nThreat Protection: B·∫£o v·ªá ch·ªëng l·∫°i c√°c cu·ªôc t·∫•n c√¥ng nh∆∞ DDoS, SQL injection, XSS\u0026hellip; Rate Limiting: Gi·ªõi h·∫°n t·ªëc ƒë·ªô truy c·∫≠p API theo IP ho·∫∑c theo ng∆∞·ªùi d√πng. Authentication: C∆° ch·∫ø x√°c th·ª±c m·∫°nh m·∫Ω, h·ªó tr·ª£ OAuth2, JWT, SSO\u0026hellip; Authorization: Ph√¢n quy·ªÅn chi ti·∫øt theo vai tr√≤, nh√≥m ng∆∞·ªùi d√πng. Monitoring \u0026amp; Logging: Gi√°m s√°t th·ªùi gian th·ª±c, alert, truy v·∫øt s·ª± c·ªë. Operational Readiness: Tri·ªÉn khai, b·∫£o tr√¨ v√† qu·∫£n l√Ω v·∫≠n h√†nh thu·∫≠n ti·ªán. Developer Integration: H·ªó tr·ª£ t·ªët cho vi·ªác t√≠ch h·ª£p frontend/backend v√† CI/CD. M·ª•c ti√™u tri·ªÉn khai H∆∞·ªõng d·∫´n s·∫Ω t·∫≠p trung v√†o vi·ªác c·∫•u h√¨nh v√† tri·ªÉn khai c√°c d·ªãch v·ª• native c·ªßa AWS ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c c√°c y√™u c·∫ßu sau:\nY√™u c·∫ßu k·ªπ thu·∫≠t D·ªãch v·ª• s·ª≠ d·ª•ng tr√™n AWS Threat Protection AWS Shield, AWS WAF DNS Protection + Entry Point Amazon Route 53 API Gateway Management Amazon API Gateway Authentication / Authorization Amazon Cognito, JWT, IAM Rate Limiting AWS WAF Rate-based rules, API Gateway quotas Business Logic AWS Lambda Data Storage Amazon S3, DynamoDB, Aurora Serverless Monitoring / Alerting Amazon CloudWatch, X-Ray Ki·∫øn tr√∫c h·ªá th·ªëng H·ªá th·ªëng ƒë∆∞·ª£c thi·∫øt k·∫ø theo h∆∞·ªõng zero-trust, v·ªõi c√°c l·ªõp b·∫£o v·ªá theo chi·ªÅu s√¢u t·ª´ l·ªõp bi√™n (network) ƒë·∫øn ·ª©ng d·ª•ng v√† d·ªØ li·ªáu.\nN·ªôi dung blog g·ªìm 3 ph·∫ßn ch√≠nh Gi·ªõi thi·ªáu ƒë·ªÅ t√†i (b·∫°n ƒëang xem) H∆∞·ªõng d·∫´n tri·ªÉn khai chi ti·∫øt tr√™n AWS Console: C·∫•u h√¨nh t·ª´ng th√†nh ph·∫ßn nh∆∞ Shield, WAF, Cognito, API Gateway, v.v. K·∫øt n·ªëi v√† t√≠ch h·ª£p gi·ªØa c√°c d·ªãch v·ª•. D·ªçn d·∫πp t√†i nguy√™n sau tri·ªÉn khai: H∆∞·ªõng d·∫´n x√≥a c√°c d·ªãch v·ª• ƒë√£ s·ª≠ d·ª•ng ƒë·ªÉ tr√°nh ph√°t sinh chi ph√≠. Y√™u c·∫ßu tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu M·ªôt t√†i kho·∫£n AWS v·ªõi quy·ªÅn qu·∫£n tr·ªã ho·∫∑c IAM ƒë·ªß quy·ªÅn thao t√°c. Ki·∫øn th·ª©c c∆° b·∫£n v·ªÅ REST API, b·∫£o m·∫≠t web (JWT, OAuth2, IAM). M·ªôt t√™n mi·ªÅn n·∫øu b·∫°n mu·ªën c·∫•u h√¨nh v·ªõi Route 53 v√† CloudFront. C√†i ƒë·∫∑t s·∫µn AWS CLI n·∫øu mu·ªën thao t√°c k·∫øt h·ª£p terminal. K·∫øt lu·∫≠n H∆∞·ªõng d·∫´n n√†y ph√π h·ª£p cho c·∫£:\nNh√† ph√°t tri·ªÉn ƒëang x√¢y d·ª±ng h·ªá th·ªëng API tr√™n AWS DevOps ho·∫∑c Security Engineer tri·ªÉn khai m√¥ h√¨nh b·∫£o m·∫≠t ph√¢n l·ªõp H·ªçc vi√™n ho·∫∑c k·ªπ s∆∞ mu·ªën t√¨m hi·ªÉu ki·∫øn tr√∫c b·∫£o m·∫≠t API hi·ªán ƒë·∫°i "
},
{
	"uri": "//localhost:1313/4-frontendintergrationwithfrontend/4.2-configapigateway/",
	"title": "Config API Gateway",
	"tags": [],
	"description": "",
	"content": "Update Lambda function Open AWS Lambda Console Select upload_documents function Comment on line 13 and uncomment line 12 Click Deloy Config API Gateway Open Amazon API Gateway Console\nClick Create API Scroll down to REST API section, click Build Leave the default REST for the protocol.\nSelect New API Enter API name: fcj-dms-api-5801 Select Endpoint type as Regional Click Create API Select API Gateway created Click Create Resource to create resource for API. Enter resource name: docs, then click Create Resource Select docs resource, then click Create Method Set up the method as follows:\nKeep default Integration type as Lambda Function Check to Lambda Proxy integration Select the region of the Lambda function you created Select upload_document function Finally press Save After created POST method, click Create Resource to create next resource. Enter resource name: id and resource path as {id}, then click Create Resource Select {id} resource, then click Create Method Set up the method as follows:\nKeep default Integration type as Lambda Function Check to Lambda Proxy integration Select the region of the Lambda function you created Select list_documents function Finally press Save Click Create Method to create a new method Set up the method as follows:\nKeep default Integration type as Lambda Function Check to Lambda Proxy integration Select the region of the Lambda function you created Select delete_document function Finally press Save Select DELETE method, then click Edit in Method request settings Expand URL Query String Parameters section, click Add query string to add a new parameter\nEnter parameter name: file. This parameter is value of of the name of the file you want to delete. Click Save Select docs resource, then click Enable CORS Check to POST, then click Save Select {id} resource, then click Enable CORS Check to GET and DELETE, then click Save After completing the setup, we deploy the API. Select /docs, then click Deploy API Select [New Stage]\nEnter stage name: dev Click Deploy Note down the URL of the API used for the next section. Expand the stage, select the POST method and write down the URL. Expand the stage, select the DELETE method and write down the URL. Expand the stage, select the GET method and write down the URL. You have finished setting up the API. Next, we will test the API working and integrate it into our application.\n"
},
{
	"uri": "//localhost:1313/2-deloydatabase/2.2-createlambdafunctions/2.2.2-createcreatingfunction/",
	"title": "Create creating function",
	"tags": [],
	"description": "",
	"content": "This section will create a function to add document information stored in the DynamoDB table.\nOpen AWS Lambda console Click Create function Enter function name: upload_document Select Python 3.9 for Runtime Click Create function Enter the following code for the lambda_function.py file: import json import boto3 import os from datetime import datetime, timezone dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) def lambda_handler(event, context): table_name = os.environ[\u0026#39;TABLE_NAME\u0026#39;] now = datetime.now(tz=timezone.utc) dt_string = now.strftime(\u0026#34;%d/%m/%Y %H:%M:%S\u0026#34;) #doc_data = json.loads(event[\u0026#34;body\u0026#34;]) doc_data = event[\u0026#34;body\u0026#34;] path = \u0026#34;protected/{}/{}\u0026#34;.format(doc_data[\u0026#39;identityId\u0026#39;], doc_data[\u0026#39;file\u0026#39;]) doc_data.update({\u0026#34;path\u0026#34;: path, \u0026#34;modified\u0026#34;: dt_string}) table = dynamodb.Table(table_name) table.put_item(Item = doc_data) # TODO implement return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;successfully upload!\u0026#39;, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#34;Access-Control-Allow-Headers\u0026#34;: \u0026#34;Access-Control-Allow-Headers, Origin, Accept, X-Requested-With, Content-Type, Access-Control-Request-Method,X-Access-Token, XKey, Authorization\u0026#34;, \u0026#34;Access-Control-Allow-Origin\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Access-Control-Allow-Methods\u0026#34;: \u0026#34;GET,PUT,POST,DELETE,OPTIONS\u0026#34; } } Then click Deloy The above code executes to get the user‚Äôs TABLE_NAME and id environment variables from the event. Then query to the DynamoDB table provided that the value of Partition key is equal to the user‚Äôs id. Then reformat the data returned after the query.\nWe need to add an environment variable to the function. Click the Configuration tab, then select Environment variables in the left menu. Press Edit Click Add environment variable Enter TABLE_NAME as key Enter the DynamoDB table name that you just created Click Save Next, add permissions for function to access DynamoDB table Click Permission on the left menu Click on the execution role of the function Expand the AWSLambdaBasicExecutionRole‚Ä¶ policy, then click Edit Click JSON. Copy the JSON below into the editor ,\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;dynamoDB:PutItem\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:REGION:ACCOUNT_ID:table/Documents\u0026#34;\r} Replace REGION and ACCOUNT_ID with the region you create the table and your account id.\nClick Review policy Click Save changes "
},
{
	"uri": "//localhost:1313/2-deloydatabase/2.2-createlambdafunctions/",
	"title": "Create Lambda function",
	"tags": [],
	"description": "",
	"content": "Content Create listing function Create creating function Create deleting function "
},
{
	"uri": "//localhost:1313/2-deloydatabase/",
	"title": "Deloy database",
	"tags": [],
	"description": "",
	"content": "Content Create DynamoDB table Build and push image docker ECS Task Definition "
},
{
	"uri": "//localhost:1313/3-configcognito/3.2-preparation/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "We perform the following steps to prepare for authentication and save files with the Amplify library in the following section:\nTo install Amplify CLI, run the command below: npm install -g @aws-amplify/cli You must install NodeJs before installing Amplify CLI You should create a user and configure an AWS profile with credentials on your machine.\nRun the below commands to clone the application to your device: git clone https://github.com/AWS-First-Cloud-Journey/FCJ-Serverless-DMS\rcd FCJ-Serverless-DMS\rnpm install Open the project and open src/component/Home/Upload.js file. Comment the block codes that call API to interact with DynamoDB To initialize Amplify for your application, run the following command from the application‚Äôs root directory:\namplify init Enter the following information:\n? Enter a name for the project `fcjdms`` The following configuration will be applied:\nProject information\n| Name: fcjdms\n| Environment: dev\n| Default editor: Visual Studio Code\n| App type: javascript\n| Javascript framework: react\n| Source Directory Path: src\n| Distribution Directory Path: build\n| Build Command: npm run-script build\n| Start Command: npm run-script start\n? Initialize the project with the above configuration? Yes\nUsing default provider awscloudformation\n? Select the authentication method you want to use: AWS profile\nFor more information on AWS Profiles, see:\nhttps://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html\n? Please choose the profile you want to use default\n? Help improve Amplify CLI by sharing non sensitive configurations on failures (y/N) ‚Ä∫ No\nOpen CloudFormation console\nSelect Stacks on the left menu, you will see the newly created stack. Click the Resources tab and you will see the resources that Amplify creates "
},
{
	"uri": "//localhost:1313/3-configcognito/3.3-authenticationandstorage/3.3.2-storagewithamplify/",
	"title": "Storage with Amplify",
	"tags": [],
	"description": "",
	"content": "After you have successfully created an account with Cognito User Pool, we will use that account to upload files to S3 bucket with Amplify in this section.\nPress the combination Ctrl+C in terminal or command line\nRun the below command at the root of the application you cloned to add storage to the application:\namplify add storage Select and enter follow the below informations:\n? Please select from one of the below mentioned services: Content (Images, audio, video, etc.) Provide a friendly name for your resource that will be used to label this cateogry in the project: fcjdmsstore Provide bucket name: fcjdmsstore Who should have access: Auth users only What kind of access do you want for Authenticated user? ·∫§n t·ªï h·ª£p Ctrl + A Do you want to add a Lambda Trigger for your S3 Bucket? no Run the following command to update cloud resources: amplify push Navigate to the CloudFormation console to check if the stack has been created. Click on a bucket‚Äôs name to open the S3 bucket‚Äôs dashboard Run the following command to start the application: npm start Click Upload Click Add files and select the files you want to upload Add tags to files or can be omitted. Then press Upload You have successfully uploaded your files Back to the S3 bucket dashboard, check if the files have been uploaded. You are done with user authentication and file upload to S3 with Amplify. The S3 bucket has created a protected folder because our application chooses Access Level as protected. To learn more about Access Level, go to the next section.\n"
},
{
	"uri": "//localhost:1313/3-configcognito/3.3-authenticationandstorage/",
	"title": "Authentication and Storage",
	"tags": [],
	"description": "",
	"content": "This is the main part of this workshop, you will add user authentication and storage with Amplify to project.\nContent Authentication with amplify Storage with amplify "
},
{
	"uri": "//localhost:1313/3-configcognito/",
	"title": "Config Cognito",
	"tags": [],
	"description": "",
	"content": "Overview In this series, we will use the Amplify library to authenticate users with Amazon Cognito, uploading files to the S3 bucket.\nContent Preparation Authentication and Storage Access level "
},
{
	"uri": "//localhost:1313/2-deloydatabase/2.2-createlambdafunctions/2.2.3-createdeletingfunction/",
	"title": "Create deleting function",
	"tags": [],
	"description": "",
	"content": "In this section, we will create a function to delete document information stored in the DynamoDB table by user id and filename.\nOpen AWS Lambda console Click Create function Enter function name: delete_documents Select Python 3.9 for Runtime Click Create function Enter the following code for the lambda_function.py file: import json import boto3 import os client = boto3.resource(\u0026#39;dynamodb\u0026#39;) def lambda_handler(event, context): # TODO implement table_name = os.environ[\u0026#39;TABLE_NAME\u0026#39;] error = None doc_pk = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] print(\u0026#34;doc_pk \u0026#34;, doc_pk) doc_sk = event[\u0026#39;queryStringParameters\u0026#39;][\u0026#39;file\u0026#39;] print(\u0026#34;doc_sk \u0026#34;, doc_sk) table = client.Table(table_name) key = { \u0026#39;user_id\u0026#39;:doc_pk, \u0026#39;file\u0026#39;: doc_sk } try: table.delete_item(Key = key) except Exception as e: error = e except Exception as e: error = e if error is None: message = \u0026#39;delete document successful!\u0026#39; else: print(error) message = \u0026#39;delete document fail\u0026#39; return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: message, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, } Then click Deloy The above code executes to get the user‚Äôs TABLE_NAME and id environment variables from the event. Then query to the DynamoDB table provided that the value of Partition key is equal to the user‚Äôs id. Then reformat the data returned after the query.\nWe need to add an environment variable to the function. Click the Configuration tab, then select Environment variables in the left menu. Press Edit Click Add environment variable Enter TABLE_NAME as key Enter the DynamoDB table name that you just created Click Save Next, add permissions for function to access DynamoDB table Click Permission on the left menu Click on the execution role of the function Expand the AWSLambdaBasicExecutionRole‚Ä¶ policy, then click Edit Click JSON. Copy the JSON below into the editor ,\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;dynamodb:DeleteItem\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:dynamodb:REGION:ACCOUNT_ID:table/Documents\u0026#34;\r} Replace REGION and ACCOUNT_ID with the region you create the table and your account id.\nClick Review policy Click Save changes "
},
{
	"uri": "//localhost:1313/4-frontendintergrationwithfrontend/4.3-testapiwithpostman/",
	"title": "Test API with Postman",
	"tags": [],
	"description": "",
	"content": "In this step, we will test operation of the APIs using Postman tool.\nTest the listing API Create new collection, then click Bank collection Enter collection name, such as: fcjdmswebstore-5801\nClick Add request Select GET method Replace {id} with abcd1234 Click Send Successful results Test the creating API Similarly add new request Select POST method Enter URL of the writing API that recorded from the previous step In Body pattern, select raw Copy the below text block: {\r\u0026#34;user_id\u0026#34;: \u0026#34;abcd1234\u0026#34;,\r\u0026#34;file\u0026#34;: \u0026#34;flowers.png\u0026#34;,\r\u0026#34;folder\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;identityId\u0026#34;: \u0026#34;123456cvbn\u0026#34;,\r\u0026#34;modified\u0026#34;: \u0026#34;21-03-2023\u0026#34;,\r\u0026#34;size\u0026#34;: \u0026#34;32KB\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;png\u0026#34;,\r\u0026#34;tag\u0026#34;: \u0026#34;image\u0026#34;\r} Click Send Open the console of DynamoDB, select Documents and select the Explore items table to check the results: Test the deleting API Similarly add new request Select DELETE method Enter URL of the writing API that recorded from the previous step, replace {id} with abcd1234 In the Params section, enter file for the key and flowers.png for the value Click Send Successful results Back to the Documents table, click the Refresh button to see the results "
},
{
	"uri": "//localhost:1313/2-deloydatabase/2.3-testlambdafunctions/",
	"title": "Test lambda function",
	"tags": [],
	"description": "",
	"content": "In this section we will create tests to see if the functions are working properly.\nTo test the functions, download the following file to your computer and run the command: aws dynamodb batch-write-item --request-items file://documentData.json\nüìé Document Data\ndocumentData.json (3 KB) Test listing function Open the list_documents function console Click Test tab Enter tc_1 for event name Enter the below json for Event JSON { \u0026#34;pathParameters\u0026#34;: {\r\u0026#34;id\u0026#34;: \u0026#34;abcd1234\u0026#34;\r}\r} Click Save, then click Test You will get all the information of the user\u0026rsquo;s files with the id abcd1234 Test creating function Open the upload_document function console Click Test tab Enter tc_1 for event name Enter the below json for Event JSON {\r\u0026#34;body\u0026#34;:{\r\u0026#34;user_id\u0026#34;: \u0026#34;abcd1234\u0026#34;,\r\u0026#34;file\u0026#34;: \u0026#34;aws_serverless.doc\u0026#34;,\r\u0026#34;folder\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;identityId\u0026#34;: \u0026#34;123456cvbn\u0026#34;,\r\u0026#34;modified\u0026#34;: \u0026#34;13-03-2023\u0026#34;,\r\u0026#34;size\u0026#34;: \u0026#34;2MB\u0026#34;,\r\u0026#34;type\u0026#34;: \u0026#34;doc\u0026#34;,\r\u0026#34;tag\u0026#34;: \u0026#34;aws, serverless\u0026#34;\r}\r} Click Save, then click Test You will get a return result of succeeded Open Documents table to check if added successfully Test deleting function Open the delete_documents function console Click Test tab Enter tc_1 for event name Enter the below json for Event JSON {\r\u0026#34;pathParameters\u0026#34;: {\r\u0026#34;id\u0026#34;: \u0026#34;abcd1234\u0026#34;\r},\r\u0026#34;queryStringParameters\u0026#34;: {\r\u0026#34;file\u0026#34;: \u0026#34;aws-exports.js\u0026#34;\r}\r} Click Save, then click Test You will get a return result of succeeded Open Documents table to check if added successfully You are done creating Lambda functions that interact with DynamoDB. In the next post we will authenticate to the archive with the Amplify library.\n"
},
{
	"uri": "//localhost:1313/3-configcognito/3.4-accesslevel/",
	"title": "Access level",
	"tags": [],
	"description": "",
	"content": "When uploading files to S3 bucket, we have 3 levels of access: public, protected and private:\nPublic: Accessible by all users of your app. Files are stored under the public/ path in your S3 bucket. Protected: Readable by all users, but writable only by the creating user. Files are stored under protected/{user_identity_id}/ where the user_identity_id corresponds to the unique Amazon Cognito Identity ID for that user. Private: Only accessible for the individual user. Files are stored under private/{user_identity_id}/ where the user_identity_id corresponds to the unique Amazon Cognito Identity ID for that user. In this section we will change the user permission to upload files to S3.\nOpen Cognito console\nClick Identity pools on the left menu Select fcjdms\u0026hellip;identitypool\u0026hellip; Click User access tab and note down the name of the Authenticated role Open IAM Role\nSelect Roles on the left menu Enter name of Authenticated role and click to searched role Expand policies to view user permissions Select Protected_policy_\u0026hellip; policy\nClick Remove We will remove the access level permission protected because the application is using that level.\nEnter policy name and click Delete You have successfully removed. Back in the application, click Add files and select the file you want to upload. Then click Upload Access to Inspect | Console mode. We received an error. Re-add permissions for the user.\nClick Add permissions Select Create inline policy Select S3 for service In Actions | Read section, select GetObject In Actions | Write section, select PutObject and DeleteObject In Resources secttion, click Add ARN Enter ARN: arn:aws:s3:::YOUR_BUCKET-dev/protected/${cognito-identity.amazonaws.com:sub}/* Replace YOUR_BUCKET with the bucket name you created earlier.\nClick Add Click Review policy Enter policy name: Protected_policy. Then click Create policy Go back to the web app, reload the file you just failed\nClick Add files, select the file you want to download Click Upload Open the console of the S3 bucket to see if the file has loaded successfully. "
},
{
	"uri": "//localhost:1313/4-frontendintergrationwithfrontend/",
	"title": "Front-end intergration with gateway",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/4-frontendintergrationwithfrontend/4.4-testapiwithfrontend/",
	"title": "Test APIs with Front-end",
	"tags": [],
	"description": "",
	"content": "After testing that the APIs work properly with Postman, we will test the APIs that are called with the front-end built from part 2.\nOpen constant.js in the root folder of project Change value of APP_API_URL with your URL: Save file Run the command lines under here: yarn build\raws s3 cp build s3://BUCKET_NAME --recursive Replace BUCKET_NAME with the bucket name you created in part 1.\nGo back to the web application in part 1. Log in with the account you registered in workshop 2. Click Upload Click Add files Select the files you want to upload Can enter tag or ignore Click Upload Return to the DynamoDB dashboard and click the Refresh icon to see the results. Open the console of the S3 bucket, check if the file has been uploaded. Return to the application, and select the My Document tab on the left menu. You will see a list of files that have just been uploaded. Click Choose to switch to delete mode Select the file you want to delete Click Delete Click OK to confirm deleting File has been deleted Check Documents table "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]